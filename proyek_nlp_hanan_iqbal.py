# -*- coding: utf-8 -*-
"""Proyek NLP_Hanan Iqbal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AAoltO397XzV199tn7PcfX7ca6oL-U0

- Nama: Hanan Iqbal Alrahma
- Submisi NLP
- Dataset: [Emotion for NLP](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp/code?datasetId=605165&sortBy=voteCount)
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import re
import time
import tensorflow as tf


from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense

data_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/Dataset emotions/train.txt',sep=";", names=["Description","Emotion"])

data_train.head()

print(data_train.shape)

data_train['Emotion'].value_counts()

"""##Mengecek data"""

data_train.Emotion.value_counts() / data_train.shape[0] *100

plt.figure(figsize=(8,4))
sns.countplot(x='Emotion', data = data_train)

"""##Menghilangkan duplicated value"""

index = data_train[data_train.duplicated() == True].index
data_train.drop(index, axis = 0, inplace = True)
data_train.reset_index(inplace=True, drop = True)

data_train[data_train['Description'].duplicated() == True]

"""##Menghilangkan duplicated text"""

index = data_train[data_train['Description'].duplicated() == True].index
data_train.drop(index, axis = 0, inplace = True)
data_train.reset_index(inplace=True, drop = True)

"""##Menghitung stopword"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

temp =data_train.copy()
stop_words = set(stopwords.words("english"))
temp['stop_words'] = temp['Description'].apply(lambda x: len(set(x.split()) & set(stop_words)))
temp.stop_words.value_counts()

"""##Cleaning data"""

from nltk.stem import SnowballStemmer, WordNetLemmatizer
nltk.download('wordnet')
def lemmatization(text):
    lemmatizer= WordNetLemmatizer()

    text = text.split()

    text=[lemmatizer.lemmatize(y) for y in text]

    return " " .join(text)

def remove_stop_words(text):

    Text=[i for i in str(text).split() if i not in stop_words]
    return " ".join(Text)

def Removing_numbers(text):
    text=''.join([i for i in text if not i.isdigit()])
    return text

def lower_case(text):

    text = text.split()

    text=[y.lower() for y in text]

    return " " .join(text)

def Removing_punctuations(text):
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)
    text = text.replace('؛',"", )

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    text =  " ".join(text.split())
    return text.strip()

def Removing_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_small_sentences(df):
    for i in range(len(df)):
        if len(df.text.iloc[i].split()) < 3:
            df.text.iloc[i] = np.nan

def normalize_text(df):
    df.Description=df.Description.apply(lambda text : lower_case(text))
    df.Description=df.Description.apply(lambda text : remove_stop_words(text))
    df.Description=df.Description.apply(lambda text : Removing_numbers(text))
    df.Description=df.Description.apply(lambda text : Removing_punctuations(text))
    df.Description=df.Description.apply(lambda text : Removing_urls(text))
    df.Description=df.Description.apply(lambda text : lemmatization(text))
    return df

def normalized_sentence(sentence):
    sentence= lower_case(sentence)
    sentence= remove_stop_words(sentence)
    sentence= Removing_numbers(sentence)
    sentence= Removing_punctuations(sentence)
    sentence= Removing_urls(sentence)
    sentence= lemmatization(sentence)
    return sentence

data_train = normalize_text(data_train)

data_train

"""##One-hot-encoding"""

cat_emotion = pd.get_dummies(data_train.Emotion)
new_data_train = pd.concat([data_train, cat_emotion], axis=1)
new_data_train = new_data_train.drop(columns='Emotion')
new_data_train

description = new_data_train['Description'].values
emotion = new_data_train[['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']].values
desc_train, desc_test, emot_train, emot_test = train_test_split(description, emotion, test_size=0.2)

emotion

description

print("Shape of desc_train: ", desc_train.shape)
print("Shape of desc_test: ", desc_test.shape)

"""##Tokenizer"""

max_len = max([len(t) for t in data_train['Description']])
max_len

tok_oov = "UNK"
trunc_type = "pre"

tokenizer = Tokenizer(num_words=10000, oov_token=tok_oov)
tokenizer.fit_on_texts(desc_train)
tokenizer.fit_on_texts(desc_test)

sekuens_latih = tokenizer.texts_to_sequences(desc_train)
sekuens_test = tokenizer.texts_to_sequences(desc_test)

padded_latih = pad_sequences(sekuens_latih, maxlen=max_len, truncating=trunc_type)
padded_test = pad_sequences(sekuens_test, maxlen=max_len, truncating=trunc_type)

print(padded_test.shape)

vocabSize = len(tokenizer.index_word) + 1
print(f"Vocabulary size = {vocabSize}")

"""##Membangun model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocabSize, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(64, dropout=0.2),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)
optimizer_adam = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(loss='categorical_crossentropy',optimizer = optimizer_adam,metrics = ['accuracy'])
model.summary()

"""##Training"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90):
      self.model.stop_training = True
      print("\n akurasi telah terpenuhi")
callbacks = myCallback()

start_time = time.time()
num_epochs = 30
history = model.fit(padded_latih,
                    emot_train,
                    epochs=num_epochs,
                    validation_data=(padded_test, emot_test),
                    callbacks = [callbacks],
                    verbose=2)
end_time = time.time()

training_time = end_time - start_time
training_time_minute = training_time / 60
print("Waktu training: {:.2f} detik".format(training_time))
print("Waktu training (menit): {:.2f} menit".format(training_time_minute))

"""##Grafik"""

accuracy = history.history['accuracy']
validation = history.history['val_accuracy']
loss = history.history['loss']
validation_loss = history.history['val_loss']

epochs = range(1, len(accuracy) + 1)

# Plot grafik akurasi
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, accuracy, 'b', label='Training Accuracy')
plt.plot(epochs, validation, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


#plot grafik loss
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training loss')
plt.plot(epochs, validation_loss, label='Training validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()